{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T6LB-X_5Fsb"
      },
      "source": [
        "# URL Vector Database Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhjkJHy_mwWV"
      },
      "source": [
        "## Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3y-QT5IWmgAk"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install langchain\n",
        "!pip install tiktoken\n",
        "!pip install faiss-cpu\n",
        "!pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErX79b3foPBz"
      },
      "source": [
        "## Specify main URL sources\n",
        "\n",
        "For this specific project, our objective is to build a vector database by extracting data from www.telkom.co.id and all of its subdirectories. There are three primary parent URLs from which we will retrieve all of their respective subdirectories. These parent URLs are:\n",
        "\n",
        "- https://www.telkom.co.id/sites/about-telkom/id_ID\n",
        "- https://www.telkom.co.id/sites/enterprise/id_ID\n",
        "- https://www.telkom.co.id/sites/wholesale/id_ID\n",
        "\n",
        "To accomplish this task, we will utilize the `BeautifulSoup` library to extract all the links or href tags from the mentioned parent URLs. This process will compile a list of all subdirectory URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "5Ro9NUNHmvpI"
      },
      "outputs": [],
      "source": [
        "main_url = [\"https://www.telkom.co.id/sites/about-telkom/id_ID\", \"https://www.telkom.co.id/sites/enterprise/id_ID\", \"https://www.telkom.co.id/sites/wholesale/id_ID\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "lM-Pv7_znC44"
      },
      "outputs": [],
      "source": [
        "subdirectories = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "omt1xxqYm-4P"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "for url in (main_url):\n",
        "\n",
        "  #Send an HTTP GET request to the main URL\n",
        "  response = requests.get(url)\n",
        "\n",
        "  # Check if the request was successful (status code 200)\n",
        "  if response.status_code == 200:\n",
        "      # Parse the HTML content of the page\n",
        "      soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "      # Find all anchor (a) tags that contain href attributes\n",
        "      links = soup.find_all(\"a\", href=True)\n",
        "\n",
        "      # Loop through the links and extract subdirectories\n",
        "      for link in links:\n",
        "          # Get the href attribute of the link\n",
        "          href = link[\"href\"]\n",
        "\n",
        "          # Join the URL with the main domain to create an absolute URL\n",
        "          absolute_url = urljoin(url, href)\n",
        "\n",
        "          # Check if the URL is a subdirectory of the main domain\n",
        "          if absolute_url.startswith(url) and absolute_url != url:\n",
        "              subdirectories.append(absolute_url)\n",
        "  else:\n",
        "      print(\"Failed to retrieve the main URL.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "B6cqKiIznzTW"
      },
      "outputs": [],
      "source": [
        "subdirectories = list(set(subdirectories))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df_y8IJvnYEy",
        "outputId": "3d604a04-5682-43d6-d5a5-2ba610a8d8b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First five url:\n",
            "https://www.telkom.co.id/sites/about-telkom/id_ID/news/batic-2023-transformasi-dan-inovasi-jadi-strategi-jitu-di-tengah-evolusi-teknologi-digital-yang-dinamis-2101\n",
            "https://www.telkom.co.id/sites/wholesale/id_ID/page/homepage-network-connectivity-973\n",
            "https://www.telkom.co.id/sites/enterprise/id_ID/page/digital-financial-banking-solution-809\n",
            "https://www.telkom.co.id/sites/about-telkom/id_ID/page/ir-informasi-atau-fakta-material-lain-174\n",
            "https://www.telkom.co.id/sites/enterprise/id_ID/news/partisipasi-telkomgroup-dukung-pendanaan-startup-nasional-melalui-peresmian-merah-putih-fund-2105\n",
            "\n",
            "Total url: 141\n"
          ]
        }
      ],
      "source": [
        "print(\"First five url:\")\n",
        "for i in range (5):\n",
        "  print(subdirectories[i])\n",
        "\n",
        "print(f\"\\nTotal url: {len(subdirectories)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v75OWnaAoLzU"
      },
      "source": [
        "## Extract Text from HTML Webpages and Turn it into document format\n",
        "\n",
        "In the next phase of our project, we will be employing the `WebBaseLoader` tool provided by LangChain. This tool will facilitate the extraction of text content from HTML webpages and convert it into a document format suitable for downstream use. Then, we will segment the document into chunks of text to avoid token limit from OpenAI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "UfnuSbwloBMW"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(subdirectories)\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "bq2y-oFwoKLL"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU3suv9loghV",
        "outputId": "c389edb7-e784-49b5-d775-0e6c05d2520e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 5738, which is longer than the specified 850\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1020, which is longer than the specified 850\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 914, which is longer than the specified 850\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 980, which is longer than the specified 850\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 4278, which is longer than the specified 850\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "chunk_size_limit = 850\n",
        "max_chunk_overlap = 20\n",
        "\n",
        "# text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=25)\n",
        "text_splitter = CharacterTextSplitter(chunk_size=chunk_size_limit, chunk_overlap=max_chunk_overlap)\n",
        "docs = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI0YIYpnojiX",
        "outputId": "b627a656-f272-4d6b-9b06-471ce6b119c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First content of our data:\n",
            " page_content='Telkom | BATIC 2023: Transformasi dan Inovasi Jadi Strategi Jitu di Tengah Evolusi Teknologi Digital yang Dinamis\\n\\n\\nTentang TelkomEnterpriseWholesale\\n\\n\\nProfil\\n\\n\\nProfil dan Riwayat Singkat\\nDewan Komisaris\\nDireksi\\nStruktur Group Perusahaan\\nPenghargaan\\nAnggaran Dasar\\nLogo Telkom Indonesia\\nAsean Summit 2023\\nHut Telkom\\n\\n\\nHubungan Investor\\n\\nLaporan-Laporan\\n\\n\\nInformasi Saham dan Obligasi\\n\\n\\nBerita dan Kegiatan\\n\\n\\nInformasi Lainnya\\n\\n\\nLaporan SEC\\n\\n\\nLaporan Keuangan\\n\\n\\nLaporan Tahunan\\n\\n\\nInfo Memo\\n\\n\\nLaporan Keberlanjutan\\n\\n\\nIkhtisar Keuangan\\n\\n\\nHarga dan Volume Saham\\n\\n\\nKomposisi Pemegang Saham\\n\\n\\nKebijakan Dividen\\n\\n\\nKronologis Pencatatan Saham\\n\\n\\nInformasi Obligasi\\n\\n\\nProspektus Penawaran Umum\\n\\n\\nRUPS\\n\\n\\nKalender Investor\\n\\n\\nEarnings Call\\n\\n\\nInformasi Kepada Investor\\n\\n\\nInformasi Aksi Korporasi\\n\\n\\nInformasi atau Fakta Material Lain' metadata={'source': 'https://www.telkom.co.id/sites/about-telkom/id_ID/news/batic-2023-transformasi-dan-inovasi-jadi-strategi-jitu-di-tengah-evolusi-teknologi-digital-yang-dinamis-2101', 'title': 'Telkom | BATIC 2023: Transformasi dan Inovasi Jadi Strategi Jitu di Tengah Evolusi Teknologi Digital yang Dinamis', 'description': 'Komisaris Telkom Marcelino Pandin, Direktur Wholesale and International Services Telkom Bogi Witjaksono, dan Chief Executive Officer Telin Budi Satria Dharma Purba, resmi membuka acara Bali Annual Telkom International Conference (BATIC) 2023 yang berlangs', 'language': 'en'}\n"
          ]
        }
      ],
      "source": [
        "print(f'First content of our data:\\n {docs[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrb7vEO4oslS",
        "outputId": "9fb2a85d-766e-4c7b-9f22-6809d39de1c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total word count: 12506\n",
            "\n",
            "Estimated tokens: 25293\n",
            "\n",
            "Estimated cost of embedding: $0.0101172\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "total_word_count = sum(len(doc.page_content.split()) for doc in docs)\n",
        "total_token_count = sum(len(enc.encode(doc.page_content)) for doc in docs)\n",
        "\n",
        "print(f\"\\nTotal word count: {total_word_count}\")\n",
        "print(f\"\\nEstimated tokens: {total_token_count}\")\n",
        "print(f\"\\nEstimated cost of embedding: ${total_token_count * 0.0004 / 1000}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkPa8RZsn3vX"
      },
      "source": [
        "## Create LLM and Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "KoMsf-DbMFRI"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "embeddings_openai = OpenAIEmbeddings(openai_api_key = 'API_KEY_HERE')\n",
        "\n",
        "llm_openai = ChatOpenAI(openai_api_key='API_KEY_HERE',\n",
        "                        temperature=0\n",
        "                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpEqYHfireoZ"
      },
      "source": [
        "## Save data document as vector database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "GQ6T0xjPrf_C"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "vector_store = FAISS.from_documents(docs, embeddings_openai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "TMDidkxWriy-"
      },
      "outputs": [],
      "source": [
        "#This step will result in the creation of a folder named \"Telkom_URL_vectorstore,\" which will be our vector database.\n",
        "\n",
        "vector_store.save_local('Telkom_URL_vectorstore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoPbO_Q1-ZLc"
      },
      "source": [
        "## Save vector database to local\n",
        "\n",
        "To streamline the process and prevent the need for repetitive vector database creation from the same data source, we will execute the following program to generate a zip file of our vector database. This zip file can then be downloaded locally for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RgdrbzVhsAgN",
        "outputId": "7b7a3cd0-e591-45ba-f100-a9fa37164f84"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/Telkom_URL_vectorstore.zip'"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "folder_path = \"/content/Telkom_URL_vectorstore\"\n",
        "output_path = \"/content/Telkom_URL_vectorstore\"\n",
        "\n",
        "shutil.make_archive(output_path, 'zip', folder_path)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
